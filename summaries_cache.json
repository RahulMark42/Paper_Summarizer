{"  We rigorously establish a bipartite mutual information scaling law in natural\nlanguage that governs long-range dependencies. This scaling law, which we show\nis distinct from and scales independently of the conventional two-point mutual\ninformation, is the key to understanding long-context language modeling. Using\nthis scaling law, we formulate the Long-context Language Modeling (L$^2$M)\ncondition, which relates a model's capacity for effective long context length\nmodeling to the scaling of its latent state size for storing past information.\nOur results are validated through experiments on both transformers and state\nspace models. This work establishes a theoretical foundation that guides the\ndevelopment of large language models toward longer context lengths.\n": "This research paper establishes a new scaling law for bipartite mutual information in natural language, specifically addressing long-range dependencies.  This law, distinct from the standard two-point mutual information, shows how mutual information scales with distance between words in a sentence.  Crucially, this scaling is independent of the two-point mutual information.\n\nThe key idea is that this bipartite mutual information scaling law is fundamental to understanding long-context language modeling.  The authors use this law to formulate the **Long-context Language Modeling (L\u00b2M) condition**.  This condition links a model's ability to handle long contexts to the scaling of its latent state size (the amount of information it can store about the past).  Essentially,  L\u00b2M  posits a relationship between the model's capacity for long-range dependencies and the resources it allocates to store relevant past information.\n\nThe authors validate their theoretical findings through experiments on transformer and state-space models.  The paper's main contribution is a theoretical framework guiding the development of large language models capable of handling significantly longer contexts, providing a principled way to understand and improve long-range dependency modeling in these models.  No specific mathematical equations or results are presented in the summary; the emphasis is on the scaling law itself and its application in the L\u00b2M condition.\n", "  Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.\n": "This research paper argues that while recent advancements in Large Language Models (LLMs) have focused on processing long *inputs*,  the equally important area of generating long *outputs* has been neglected.  The authors highlight a critical gap in current LLMs' ability to produce coherent, contextually rich, and logically consistent long-form text, which is crucial for tasks like novel writing, long-term planning, and complex reasoning.  The paper advocates for a paradigm shift in NLP research to prioritize the development of foundational LLMs specifically designed for high-quality long-output generation, emphasizing the significant potential of this under-explored area for real-world applications.  There are no specific mathematical ideas or results presented, as the paper is primarily a conceptual argument for a new research direction.\n", "  Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner.\n": "This research paper investigates whether Large Language Models (LLMs) use in-context learning (ICL) for structured reasoning in a way consistent with Bayesian inference or simply through pattern matching.  The researchers used a controlled experiment involving biased coin flips presented as few-shot examples to LLMs.  Their key findings are:\n\n1. **Biased Priors:** LLMs often start with pre-existing biases (priors) that affect their initial predictions (zero-shot setting).\n\n2. **Evidence Dominates Bias:**  In-context examples (ICL) override explicitly stated biases in the instructions.  The data presented through ICL is more influential than prior beliefs hardcoded into the model.\n\n3. **Bayesian-like Updates:** LLMs generally update their predictions in a manner consistent with Bayesian posterior updates.  Deviations from perfect Bayesian updates are primarily attributed to initially miscalibrated priors, not flaws in the update mechanism itself.\n\n4. **Attention's Limited Role:** The attention mechanism within the LLM has minimal impact on the accuracy of its Bayesian-like inference.\n\nIn essence, the study demonstrates that while LLMs exhibit inherent biases,  they can learn from and adapt to new evidence in a way that resembles Bayesian reasoning, primarily through ICL. The paper suggests that the deviation from ideal Bayesian inference stems mainly from initial biases rather than a fundamental lack of Bayesian-like updating capabilities.  No specific mathematical formulas are presented in the summary, but the core idea revolves around the comparison of LLM predictions with Bayesian posterior probabilities calculated from the provided data and the initial prior assumption.\n", "  We study reconstructing and predicting 3D fluid appearance and velocity from\na single video. Current methods require multi-view videos for fluid\nreconstruction. We present FluidNexus, a novel framework that bridges video\ngeneration and physics simulation to tackle this task. Our key insight is to\nsynthesize multiple novel-view videos as references for reconstruction.\nFluidNexus consists of two key components: (1) a novel-view video synthesizer\nthat combines frame-wise view synthesis with video diffusion refinement for\ngenerating realistic videos, and (2) a physics-integrated particle\nrepresentation coupling differentiable simulation and rendering to\nsimultaneously facilitate 3D fluid reconstruction and prediction. To evaluate\nour approach, we collect two new real-world fluid datasets featuring textured\nbackgrounds and object interactions. Our method enables dynamic novel view\nsynthesis, future prediction, and interaction simulation from a single fluid\nvideo. Project website: https://yuegao.me/FluidNexus.\n": "This research paper introduces FluidNexus, a novel framework for reconstructing and predicting the 3D appearance and velocity of fluids from a *single* video.  This is a significant advance over existing methods which typically require multiple video views.\n\n**Key Ideas and Main Points:**\n\n* **Single-view reconstruction:**  The core innovation is reconstructing 3D fluid information from the limitations of a single video input, a challenging problem previously addressed only with multi-view data.\n* **Novel-view synthesis:** FluidNexus overcomes the single-view limitation by *synthesizing* multiple novel views of the fluid. This is achieved using a two-stage process: frame-wise view synthesis followed by video diffusion refinement to enhance realism.\n* **Physics-integrated particle representation:**  The 3D reconstruction and prediction are not solely based on image information but are integrated with a physics simulation using a particle representation. This ensures physically plausible results.  The simulation and rendering are differentiable, enabling end-to-end training.\n* **Two new datasets:** The researchers created two new real-world datasets with textured backgrounds and object interactions for rigorous evaluation of the proposed method.\n\n\n**Key Mathematical Ideas/Results (Implicit):**\n\nWhile the paper doesn't explicitly detail specific mathematical formulations, the underlying mathematical concepts are implied:\n\n* **Differentiation through simulation:** The use of differentiable rendering and simulation suggests the application of automatic differentiation techniques (like backpropagation) to optimize the model parameters. This allows the system to learn the relationship between the input video and the underlying 3D fluid dynamics.\n* **Video diffusion models:** The refinement stage likely utilizes concepts from diffusion models, involving forward diffusion processes (adding noise) and reverse diffusion processes (denoising) to generate realistic videos. This involves stochastic differential equations and probabilistic inference.\n* **Particle-based fluid simulation:**  The 3D fluid representation likely uses techniques from particle-based fluid simulation (e.g., Smoothed Particle Hydrodynamics, SPH), involving numerical integration of fluid equations (Navier-Stokes, etc.)  The differentiability of the simulation implies the use of differentiable solvers or approximations.\n\n\nIn summary, FluidNexus uses a clever combination of novel-view synthesis, physics-based simulation, and differentiable rendering to achieve a significant breakthrough in 3D fluid reconstruction and prediction from limited input.  The core strength lies in bridging computer vision techniques (video generation) and physically-based simulation.\n", "  Scene flow estimation is a foundational task for many robotic applications,\nincluding robust dynamic object detection, automatic labeling, and sensor\nsynchronization. Two types of approaches to the problem have evolved: 1)\nSupervised and 2) optimization-based methods. Supervised methods are fast\nduring inference and achieve high-quality results, however, they are limited by\nthe need for large amounts of labeled training data and are susceptible to\ndomain gaps. In contrast, unsupervised test-time optimization methods do not\nface the problem of domain gaps but usually suffer from substantial runtime,\nexhibit artifacts, or fail to converge to the right solution. In this work, we\nmitigate several limitations of existing optimization-based methods. To this\nend, we 1) introduce a simple voxel grid-based model that improves over the\nstandard MLP-based formulation in multiple dimensions and 2) introduce a new\nmultiframe loss formulation. 3) We combine both contributions in our new\nmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only\nby EulerFlow among unsupervised methods while achieving comparable performance\nat a fraction of the computational cost. Floxels achieves a massive speedup of\nmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10\nminutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels\nachieves a speedup of ~14x.\n": "This research paper addresses the problem of scene flow estimation, a crucial task for robotics.  The authors focus on improving optimization-based methods, which are less reliant on large labeled datasets (unlike supervised methods) but often suffer from slow runtime and poor results.  Their contributions are threefold:\n\n1. **Voxel Grid-based Model:** They replace the standard Multilayer Perceptron (MLP) model with a simpler voxel grid representation. This improves performance across several dimensions, likely by offering better spatial reasoning and potentially reducing computational complexity.\n\n2. **Multiframe Loss Formulation:**  A novel loss function incorporating information from multiple frames is introduced. This likely enhances the accuracy and robustness of the optimization process by leveraging temporal consistency.\n\n3. **Floxels Method:**  The paper combines the voxel grid model and the new multiframe loss into a single method called Floxels.\n\n**Key Results and Mathematical Ideas:** While the paper doesn't delve into specific mathematical equations, the core idea is an improved optimization algorithm.  The improvements stem from a change in model architecture (voxel grid instead of MLP) and a refined loss function design (multiframe).  The key quantitative result is a significant speedup compared to existing methods, achieving a runtime reduction from a day to 10 minutes per sequence (a speedup of 60-140x) over EulerFlow, a state-of-the-art unsupervised method.  Compared to a faster but less accurate baseline (NSFP), Floxels achieves a 14x speedup.  Despite this massive speed increase, Floxels' accuracy on the Argoverse 2 benchmark is comparable to, and only slightly below, EulerFlow among unsupervised approaches.  This demonstrates a compelling trade-off between speed and accuracy.\n", "gsplat is an open-source library designed for training and developing Gaussian Splatting methods. It features a front-end with Python bindings compatible with the PyTorch library and a back-end with highly optimized CUDA kernels. gsplat offers numerous features that enhance the optimization of Gaussian Splatting models, which include optimization improvements for speed, memory, and convergence times. Experimental results demonstrate that gsplat achieves up to 10% less training time and 4x less memory than the original implementation. Utilized in several research projects, gsplat is actively maintained on GitHub. Source code is available at https://github.com/nerfstudio-project/gsplat under Apache License 2.0. We welcome contributions from the open-source community.": "This research paper introduces gsplat, an open-source Python library built on PyTorch and CUDA for training and developing Gaussian Splatting models.  The core contribution is a highly optimized implementation that significantly improves upon the original Gaussian Splatting method's performance.  Key improvements include:\n\n* **Faster Training:** gsplat achieves up to 10% reduction in training time compared to the original implementation.\n* **Reduced Memory Consumption:**  gsplat uses up to 4 times less memory than the original implementation.\n* **Improved Convergence:** While not explicitly quantified with a percentage, the paper implies faster convergence times due to the optimization improvements.\n\nThe key mathematical idea remains the underlying Gaussian Splatting algorithm itself, which the library efficiently implements.  The paper doesn't detail specific mathematical optimizations within the CUDA kernels, but the reported speed and memory improvements suggest algorithmic and data structure optimizations within the implementation.  The paper highlights the practical benefits of gsplat through experimental results and its use in multiple research projects, emphasizing its availability and community contribution model.\n", "We consider online statistical inference of constrained stochastic nonlinear optimization problems. We apply the Stochastic Sequential Quadratic Programming (StoSQP) method to solve these problems, which can be regarded as applying second-order Newton's method to the Karush-Kuhn-Tucker (KKT) conditions. In each iteration, the StoSQP method computes the Newton direction by solving a quadratic program, and then selects a proper adaptive stepsize $\\bar{\\alpha}_t$ to update the primal-dual iterate. To reduce dominant computational cost of the method, we inexactly solve the quadratic program in each iteration by employing an iterative sketching solver. Notably, the approximation error of the sketching solver need not vanish as iterations proceed, meaning that the per-iteration computational cost does not blow up. For the above StoSQP method, we show that under mild assumptions, the rescaled primal-dual sequence $1/\\sqrt{\\bar{\\alpha}_t}\\cdot (x_t -x^\\star, \\lambda_t - \\lambda^\\star)$ converges to a mean-zero Gaussian distribution with a nontrivial covariance matrix depending on the underlying sketching distribution. To perform inference in practice, we also analyze a plug-in covariance matrix estimator. We illustrate the asymptotic normality result of the method both on benchmark nonlinear problems in CUTEst test set and on linearly/nonlinearly constrained regression problems.": "This research paper investigates online statistical inference for constrained stochastic nonlinear optimization problems.  The authors use a Stochastic Sequential Quadratic Programming (StoSQP) method, essentially a second-order (Newton's method) approach applied to the Karush-Kuhn-Tucker (KKT) conditions.  The core innovation lies in using an *inexact* iterative sketching solver to solve the quadratic program in each StoSQP iteration, significantly reducing computational cost.  Crucially, the approximation error from this inexact solver *doesn't* need to decrease to zero over iterations, maintaining a manageable per-iteration computational burden.\n\nThe key theoretical contribution is proving that, under mild assumptions, the rescaled sequence of primal-dual iterates,  $1/\\sqrt{\\bar{\\alpha}_t}\\cdot (x_t -x^\\star, \\lambda_t - \\lambda^\\star)$ (where  $(x_t, \\lambda_t)$ are the primal-dual iterates at step *t*, and $(x^\\star, \\lambda^\\star)$ are the optimal solutions, and $\\bar{\\alpha}_t$ is an adaptive step size), converges to a zero-mean Gaussian distribution. The covariance matrix of this Gaussian distribution is non-trivial and depends on the sketching distribution used in the inexact solver.  A practical plug-in estimator for this covariance matrix is also analyzed.\n\nIn essence, the paper demonstrates that even with an inexact, computationally efficient solver, the StoSQP method yields asymptotically normal iterates, enabling statistical inference about the optimal solution. This is validated through numerical experiments on benchmark nonlinear problems (CUTEst) and constrained regression problems.  The key mathematical result is the asymptotic normality of the rescaled primal-dual iterates and the provision of a method to estimate the associated covariance matrix.\n", "While many Machine Learning methods have been developed or transposed on Riemannian manifolds to tackle data with known non-Euclidean geometry, Optimal Transport (OT) methods on such spaces have not received much attention. The main OT tool on these spaces is the Wasserstein distance, which suffers from a heavy computational burden. On Euclidean spaces, a popular alternative is the Sliced-Wasserstein distance, which leverages a closed-form solution of the Wasserstein distance in one dimension, but which is not readily available on manifolds. In this work, we derive general constructions of Sliced-Wasserstein distances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive curvature, which include among others Hyperbolic spaces or the space of Symmetric Positive Definite matrices. Then, we propose different applications such as classification of documents with a suitably learned ground cost on a manifold, and data set comparison on a product manifold. Additionally, we derive non-parametric schemes to minimize these new distances by approximating their Wasserstein gradient flows.": "This research paper introduces novel Sliced-Wasserstein (SW) distances on Cartan-Hadamard manifolds \u2013 Riemannian manifolds with non-positive curvature, like hyperbolic spaces or the space of Symmetric Positive Definite (SPD) matrices.  The key problem addressed is the high computational cost of Wasserstein distances, a common tool in Optimal Transport (OT) for comparing probability distributions, especially when dealing with non-Euclidean data.\n\n**Key Ideas & Main Points:**\n\n* **Addressing the computational bottleneck of Wasserstein distances on manifolds:**  The paper tackles the computational challenge of using Wasserstein distances on Riemannian manifolds, where standard Euclidean methods don't directly apply.\n* **Extending Sliced-Wasserstein distances to manifolds:** It proposes a generalization of the computationally efficient Sliced-Wasserstein distance (SW) from Euclidean spaces to Cartan-Hadamard manifolds. This is a significant contribution, as a closed-form solution for the 1D Wasserstein distance, crucial for SW, isn't readily available on manifolds.  The paper provides the mathematical framework for constructing these SW distances on such manifolds.\n* **Applications of the new SW distances:** The proposed SW distances are demonstrated through applications such as document classification (using a learned cost function on a manifold) and dataset comparison on product manifolds.\n* **Non-parametric minimization via gradient flows:** The paper develops non-parametric methods to minimize the newly defined SW distances on manifolds, leveraging the approximation of Wasserstein gradient flows.\n\n**Key Mathematical Ideas/Results:**\n\nThe core mathematical contribution lies in the derivation of the generalized Sliced-Wasserstein distances on Cartan-Hadamard manifolds. This involves adapting the concepts of slicing and the 1D Wasserstein distance to the curved geometry of these manifolds.  The paper doesn't explicitly state specific formulas, but implicitly demonstrates the construction of these SW distances through the presented applications.  The development of non-parametric minimization schemes using gradient flows is another key mathematical result, enabling practical application of the proposed SW distances.  The paper likely contains detailed mathematical proofs and derivations supporting these claims.\n", "The acceleration of gradient-based optimization methods is a subject of significant practical and theoretical importance, particularly within machine learning applications. While much attention has been directed towards optimizing within Euclidean space, the need to optimize over spaces of probability measures in machine learning motivates the exploration of accelerated gradient methods in this context, too. To this end, we introduce a Hamiltonian-flow approach analogous to momentum-based approaches in Euclidean space. We demonstrate that, in the continuous-time setting, algorithms based on this approach can achieve convergence rates of arbitrarily high order. We complement our findings with numerical examples.": "This research paper investigates accelerated gradient-based optimization methods for probability measures, a crucial problem in machine learning.  Unlike most existing work focusing on Euclidean spaces, this paper tackles optimization within the more complex space of probability distributions.\n\nThe key idea is to adapt the concept of \"momentum\" (commonly used in accelerated gradient descent in Euclidean space) to this new setting using a Hamiltonian flow approach.  This approach, inspired by Hamiltonian mechanics, allows the algorithm to evolve dynamically through the space of probability measures.\n\nThe main result is that this Hamiltonian-flow-based approach achieves *arbitrarily high-order convergence rates* in the continuous-time setting.  This is a significant improvement over standard gradient descent methods.  While the paper doesn't explicitly state the precise mathematical expressions for these high-order convergence rates, the implication is that the convergence is significantly faster than standard methods.\n\nFinally, the paper supports its theoretical findings with numerical examples, demonstrating the practical effectiveness of the proposed approach.  The paper does not provide specific mathematical formulas describing the Hamiltonian flow or the resulting optimization algorithm, only stating the key result of arbitrarily high-order convergence.\n", "Gaussian processes are pervasive in functional data analysis, machine learning, and spatial statistics for modeling complex dependencies. Scientific data are often heterogeneous in their inputs and contain multiple known discrete groups of samples; thus, it is desirable to leverage the similarity among groups while accounting for heterogeneity across groups. We propose multi-group Gaussian processes (MGGPs) defined over $\\mathbb{R}^p\\times \\mathscr{C}$, where $\\mathscr{C}$ is a finite set representing the group label, by developing general classes of valid (positive definite) covariance functions on such domains. MGGPs are able to accurately recover relationships between the groups and efficiently share strength across samples from all groups during inference, while capturing distinct group-specific behaviors in the conditional posterior distributions. We demonstrate inference in MGGPs through simulation experiments, and we apply our proposed MGGP regression framework to gene expression data to illustrate the behavior and enhanced inferential capabilities of multi-group Gaussian processes by jointly modeling continuous and categorical variables.": "This research paper introduces Multi-Group Gaussian Processes (MGGPs) as a novel method for modeling data with both continuous and categorical features.  The key idea is to extend Gaussian Processes (GPs), which are commonly used for modeling complex dependencies in data, to handle situations where data is naturally divided into distinct groups.  Instead of a single GP, MGGPs use a covariance function defined over a combined input space of continuous variables ($\\mathbb{R}^p$) and a categorical variable representing group membership ($\\mathscr{C}$).\n\nThe main contribution is the development of general classes of *valid* (positive definite) covariance functions on this combined input space.  This validity is crucial because it ensures the resulting covariance matrix remains positive definite, a necessary condition for a valid Gaussian process.  This allows MGGPs to:\n\n* **Leverage similarity between groups:** The model shares information across groups, improving inference efficiency and accuracy, particularly when data within some groups is sparse.\n* **Account for heterogeneity across groups:**  Despite sharing information, the model allows for distinct group-specific behaviors, captured in the conditional posterior distributions.\n\nThe paper demonstrates the effectiveness of MGGPs through:\n\n* **Simulation experiments:** These confirm the ability of MGGPs to accurately recover relationships between groups.\n* **Real-world application to gene expression data:**  This application showcases how MGGPs can effectively model data with continuous (gene expression levels) and categorical (e.g., tissue type) variables.\n\nIn essence, the paper provides a flexible and powerful extension of GPs by explicitly incorporating group structure into the covariance function, leading to improved inference and a more nuanced understanding of data with both continuous and categorical features.  No specific mathematical results beyond the establishment of valid covariance function classes are explicitly mentioned in the abstract.  The core mathematical contribution lies in the definition and validation of these covariance functions themselves, which are not detailed here.\n", "Graph neural network (GNN) models have been widely used for learning graph-structured data. Due to the permutation-invariant requirement of graph learning tasks, a basic element in graph neural networks is the invariant and equivariant linear layers. Previous work (Maron et al., 2019b) provided a maximal collection of invariant and equivariant linear layers and a simple deep neural network model, called k-IGN, for graph data defined on k-tuples of nodes. It is shown that the expressive power of k-IGN is at least as good as the  k-Weisfeiler-Leman (WL) algorithm in graph isomorphism tests. However, the dimension of the invariant layer and equivariant layer is the k-th and 2k-th bell numbers, respectively. Such high complexity makes it computationally infeasible for k-IGNs with k >= 3. In this paper, we show that a much smaller dimension for the linear layers is sufficient to achieve the same expressive power. We provide two sets of orthogonal bases for the linear layers, each with only 3(2^k-1)-k basis elements. Based on these linear layers, we develop neural network models GNN-a and GNN-b and show that for the graph data defined on k-tuples of data, GNN-a and GNN-b achieve the expressive power of the k-WL algorithm and the (k+1)-WL algorithm in graph isomorphism tests, respectively. In molecular prediction tasks on benchmark datasets, we demonstrate that low-order neural network models consisting of the proposed linear layers achieve better performance than other neural network models. In particular, order-2 GNN-b and order-3 GNN-a both have 3-WL expressive power, but use a much smaller basis and hence much less computation time than known neural network models.": "This research paper addresses the computational limitations of existing Graph Neural Networks (GNNs) designed for graph isomorphism testing and molecular prediction.  Specifically, it tackles the high dimensionality of invariant and equivariant linear layers in previous models like k-IGN, which scales with the k-th and 2k-th Bell numbers, making them impractical for k \u2265 3.\n\n**Key Ideas and Main Points:**\n\n* **Problem:** Existing high-order GNNs (those considering k-tuples of nodes) are computationally expensive due to the exponentially growing size of their linear layers.\n* **Solution:** The authors propose two new GNN architectures, GNN-a and GNN-b, utilizing significantly smaller-dimensional linear layers while retaining the same expressive power as the k-Weisfeiler-Leman (WL) algorithm.  They achieve this by constructing two sets of orthogonal bases for these layers, each containing only 3(2<sup>k</sup>-1) - k basis elements, a drastic reduction compared to the Bell numbers.\n* **Expressive Power:** GNN-a achieves the expressive power of the k-WL algorithm, while GNN-b achieves the expressive power of the (k+1)-WL algorithm.  This means they can distinguish graphs that the respective WL algorithms can distinguish.\n* **Empirical Results:** Experiments on molecular prediction benchmarks show that the proposed low-order GNNs (GNN-a and GNN-b) outperform existing GNN models.  Notably, order-2 GNN-b and order-3 GNN-a, both possessing 3-WL expressive power, are significantly faster due to the reduced dimensionality of their linear layers.\n\n\n**Key Mathematical Ideas/Results:**\n\n* The core contribution is the construction of two sets of orthogonal bases for the invariant and equivariant linear layers, each with a significantly smaller dimension (3(2<sup>k</sup>-1) - k) compared to the k-th and 2k-th Bell numbers used in previous work.\n* The paper proves that these smaller-dimensional linear layers are sufficient to achieve the expressive power of the k-WL and (k+1)-WL algorithms, respectively, for GNN-a and GNN-b.  This is a crucial theoretical result justifying the proposed architectural change.\n\n\nIn essence, the paper presents a significant advance in efficient high-order GNN design, offering a computationally feasible alternative to existing models without sacrificing expressive power, leading to improved performance in practical applications.\n", "Pearl\u2019s do calculus is a complete axiomatic approach to learn the identifiable causal effects from observational data. When such an effect is not identifiable, it is necessary to perform a collection of often costly interventions in the system to learn the causal effect. In this work, we consider the problem of designing a collection of interventions with the minimum cost to identify the desired effect. First, we prove that this problem is NP-complete and subsequently propose an algorithm that can either find the optimal solution or a logarithmic-factor approximation of it. This is done by establishing a connection between our problem and the minimum hitting set problem. Additionally, we propose several polynomial time heuristic algorithms to tackle the computational complexity of the problem. Although these algorithms could potentially stumble on sub-optimal solutions, our simulations show that they achieve small regrets on random graphs.": "This research paper tackles the problem of efficiently identifying causal effects from observational data when Pearl's do-calculus fails (i.e., the effect is not identifiable).  Since non-identifiable effects require costly interventions to learn, the paper focuses on *minimizing the cost of these interventions*.\n\n**Key Ideas and Main Points:**\n\n* **Problem Definition:** The core problem is to find the minimum-cost set of interventions needed to identify a specific causal effect.  This contrasts with simply identifying *if* an effect is identifiable.\n\n* **NP-Completeness Proof:** The authors formally prove that finding the absolute minimum-cost intervention set is an NP-complete problem. This means finding the optimal solution is computationally intractable for large systems.\n\n* **Approximation Algorithm:**  Despite the NP-completeness, they develop an algorithm that guarantees either an optimal solution or a solution within a logarithmic factor of the optimal cost.  This is achieved by leveraging a connection between the intervention selection problem and the well-studied \"minimum hitting set\" problem, for which approximation algorithms exist.\n\n* **Heuristic Algorithms:** Recognizing the computational limitations, the paper also proposes several heuristic algorithms that run in polynomial time. While these heuristics might not always find the absolute best solution, simulations demonstrate they perform well in practice (low \"regret,\" meaning the cost of their solution is close to optimal).\n\n**Key Mathematical Idea/Result:**\n\nThe central mathematical contribution is the connection established between the minimum-cost intervention selection problem and the minimum hitting set problem. This allows the authors to leverage existing approximation algorithms from the latter problem to develop an approximation algorithm for the former.  The NP-completeness proof formally establishes the inherent computational difficulty of the problem.\n", "Robustness to malicious attacks is of paramount importance for distributed learning. Existing works usually consider the classical Byzantine attacks model, which assumes that some workers can send arbitrarily malicious messages to the server and disturb the aggregation steps of the distributed learning process. To defend against such worst-case Byzantine attacks, various robust aggregators have been proposed. They are proven to be effective and much superior to the often-used mean aggregator. In this paper, however, we demonstrate that the robust aggregators are too conservative for a class of weak but practical malicious attacks, known as label poisoning attacks, where the sample labels of some workers are poisoned. Surprisingly, we are able to show that the mean aggregator is more robust than the state-of-the-art robust aggregators in theory, given that the distributed data are sufficiently heterogeneous. In fact, the learning error of the mean aggregator is proven to be order-optimal in this case. Experimental results corroborate our theoretical findings, showing the superiority of the mean aggregator under label poisoning attacks.": "This research paper challenges the common assumption that robust aggregators (designed to withstand worst-case Byzantine attacks in distributed learning) are always superior to the simple mean aggregator.  The authors focus on a more realistic, weaker attack: **label poisoning**, where attackers corrupt only the data labels at some worker nodes.\n\n**Key Ideas and Main Points:**\n\n* **Byzantine vs. Label Poisoning Attacks:** The paper contrasts the classical Byzantine attack model (arbitrary malicious messages) with label poisoning attacks (targeted data corruption).\n* **Superiority of Mean Aggregator under Label Poisoning:**  Counterintuitively, the paper demonstrates that under label poisoning attacks and sufficiently heterogeneous data, the simple mean aggregator outperforms state-of-the-art robust aggregators. This is a surprising result given the established robustness of those aggregators against Byzantine attacks.\n* **Theoretical Justification:** The authors provide a theoretical analysis proving the order-optimality of the mean aggregator's learning error under label poisoning with heterogeneous data.  This implies that the mean aggregator achieves the best possible error rate up to a constant factor.  The specific mathematical details of this order-optimality proof are not provided in the summary but are crucial to the paper's contribution.\n* **Experimental Validation:**  Empirical results are presented to support the theoretical findings, confirming the superior performance of the mean aggregator under label poisoning attacks in practice.\n\n**Key Mathematical Idea:** The core mathematical contribution lies in the theoretical proof showing the order-optimality of the mean aggregator's learning error under the specified attack and data conditions.  This demonstrates a specific bound on the error that highlights its superiority to robust aggregators in this scenario.  The exact mathematical expressions are absent from the provided abstract, but the claim of \"order-optimality\" is the key mathematical result.\n", "In this paper, we consider federated Q-learning, which aims to learn an optimal Q-function by periodically aggregating local Q-estimates trained on local data alone. Focusing on infinite-horizon tabular Markov decision processes, we provide sample complexity guarantees for both the synchronous and asynchronous variants of federated Q-learning, which exhibit a linear speedup with respect to the number of agents and near-optimal dependencies on other salient problem parameters. In the asynchronous setting, existing analyses of federated Q-learning, which adopt an equally weighted averaging of local Q-estimates, require that every agent covers the entire state-action space. In contrast, our improved sample complexity scales inverse proportionally to the minimum entry of the average stationary state-action occupancy distribution of all agents, thus only requiring the agents to collectively cover the entire state-action space, unveiling the blessing of heterogeneity. However, its sample complexity still suffers when the local trajectories are highly heterogeneous. In response, we propose a novel federated Q-learning algorithm with importance averaging, giving larger weights to more frequently visited state-action pairs, which achieves a robust linear speedup as if all trajectories are centrally processed, regardless of the heterogeneity of local behavior policies.": "This research paper analyzes the sample complexity of federated Q-learning, a method for training reinforcement learning agents collaboratively across multiple devices without centralizing data.  The authors focus on infinite-horizon tabular Markov Decision Processes (MDPs).\n\n**Key Ideas and Main Points:**\n\n* **Improved Sample Complexity Bounds:** The paper provides novel sample complexity bounds for both synchronous and asynchronous federated Q-learning.  These bounds demonstrate a **linear speedup** in the learning process with respect to the number of agents.  This means adding more agents significantly reduces the total amount of data needed to reach a solution.\n\n* **Asynchronous Federated Q-learning with Heterogeneous Agents:**  The paper addresses a limitation of existing asynchronous federated Q-learning analyses.  Previous work assumed each agent interacts with the entire state-action space. This paper shows that the sample complexity improves if the agents *collectively* cover the entire state-action space, even if individual agents only explore parts of it (\"blessing of heterogeneity\").\n\n* **Importance Averaging for Robust Linear Speedup:** The authors recognize that highly heterogeneous local trajectories can hinder the performance of even the improved asynchronous algorithm. To address this, they propose a new algorithm using **importance averaging**. This method assigns weights to local Q-estimates proportional to how frequently state-action pairs are visited.  This crucial modification achieves a robust linear speedup, essentially mimicking the performance of centrally processed data, regardless of the heterogeneity in agents' exploration.\n\n**Key Mathematical Ideas/Results:**\n\nThe core contribution lies in deriving tighter sample complexity bounds. While the exact formulas aren't provided in the summary, the key improvements are:\n\n* **Linear speedup:** The sample complexity scales proportionally to 1/N, where N is the number of agents.\n* **Dependence on state-action occupancy:**  The asynchronous algorithm's sample complexity depends inversely on the minimum entry of the average stationary state-action occupancy distribution across all agents,  allowing for heterogeneity in agent exploration.\n* **Robustness to heterogeneity (importance averaging):** The proposed importance averaging technique removes the negative impact of highly heterogeneous local trajectories on sample complexity, maintaining the linear speedup.\n\n\nIn essence, the paper advances the theoretical understanding of federated Q-learning, demonstrating its efficiency and robustness through improved sample complexity bounds and the introduction of an innovative importance-weighted averaging method.\n", "PyTorch 2.x introduces a compiler designed to accelerate deep learning programs. However, for machine learning researchers, fully leveraging the PyTorch compiler can be challenging due to its operation at the Python bytecode level, making it appear as an opaque box. To address this, we introduce depyf, a tool designed to demystify the inner workings of the PyTorch compiler. depyf decompiles the bytecode generated by PyTorch back into equivalent source code and establishes connections between the code objects in the memory and their counterparts in source code format on the disk. This feature enables users to step through the source code line by line using debuggers, thus enhancing their understanding of the underlying processes. Notably, depyf is non-intrusive and user-friendly, primarily relying on two convenient context managers for its core functionality. The project is openly available at https://github.com/thuml/depyf and is recognized as a PyTorch ecosystem project at https://pytorch.org/blog/introducing-depyf.": "This research paper introduces depyf, a tool designed to improve the understandability and usability of the PyTorch 2.x compiler.  The PyTorch compiler operates at the Python bytecode level, making its internal workings opaque to researchers.  depyf addresses this by:\n\n* **Decompiling bytecode:** It translates the PyTorch-generated bytecode back into equivalent Python source code.  This allows researchers to see the compiled code in a human-readable format.\n\n* **Connecting code objects:** depyf links the in-memory code objects to their corresponding source code files on disk. This connection is crucial for debugging and understanding the compilation process.\n\n* **Enabling debugging:** The decompiled code can be stepped through using standard debuggers, providing a line-by-line view of the compiler's actions.\n\n* **User-friendly design:** depyf is implemented using convenient context managers, making its integration into existing workflows straightforward and non-intrusive.\n\nThe key contribution is not a new mathematical algorithm or result, but a tool that significantly enhances the *understanding* and *debugging* of the PyTorch 2.x compiler.  This makes the compiler more accessible to machine learning researchers, allowing them to more effectively leverage its performance benefits.  No specific mathematical ideas or results are presented within the description of the paper itself.\n"}