{"  We rigorously establish a bipartite mutual information scaling law in natural\nlanguage that governs long-range dependencies. This scaling law, which we show\nis distinct from and scales independently of the conventional two-point mutual\ninformation, is the key to understanding long-context language modeling. Using\nthis scaling law, we formulate the Long-context Language Modeling (L$^2$M)\ncondition, which relates a model's capacity for effective long context length\nmodeling to the scaling of its latent state size for storing past information.\nOur results are validated through experiments on both transformers and state\nspace models. This work establishes a theoretical foundation that guides the\ndevelopment of large language models toward longer context lengths.\n": "This research paper establishes a novel **bipartite mutual information scaling law** for long-range dependencies in natural language.  This law describes how the mutual information between distant parts of a sentence scales with their separation.  Crucially, it's shown to be **independent** of the standard two-point mutual information, indicating a different mechanism governs long-range dependencies.\n\nThe key finding is that this bipartite mutual information scaling is fundamental to understanding long-context language modeling.  Based on this scaling law, the authors formulate the **Long-context Language Modeling (L\u00b2M) condition**.  This condition links a model's ability to handle long contexts to the scaling of its latent state size\u2014the amount of information it can store about the past.  Essentially,  the L\u00b2M condition provides a theoretical constraint on how much memory a model needs to effectively process long sentences.\n\nThe researchers validated their theoretical findings through experiments on both transformer and state-space models, demonstrating the practical relevance of their bipartite mutual information scaling law and the L\u00b2M condition.  The paper concludes by suggesting that this theoretical framework provides crucial guidance for developing future large language models capable of handling even longer context lengths.  There are no specific mathematical equations presented in the summary, as the paper focuses on the scaling *law* itself rather than specific mathematical formulas for the law.  The core mathematical idea is the relationship between bipartite mutual information, context length, and latent state size, expressed qualitatively through the L\u00b2M condition.\n", "  Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.\n": "This research paper argues that while recent progress in Large Language Models (LLMs) has focused on processing long input contexts, generating long-form outputs remains a significantly under-researched area.  The authors highlight a crucial gap:  current LLMs excel at understanding lengthy inputs but struggle to produce equally long, coherent, and contextually rich outputs.  \n\nThe key idea is to shift the focus of NLP research towards developing LLMs specifically designed for long-output generation.  This is crucial for applications requiring extended text outputs, such as novel writing, long-term planning, and complex reasoning.  The paper doesn't present specific mathematical models or results, but rather advocates for a change in research priorities, emphasizing the importance of tackling the challenges inherent in generating high-quality, long-form text as a fundamental aspect of advancing LLM capabilities.  The core argument is qualitative, focusing on the need for this under-explored area to receive dedicated attention.\n", "  Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner.\n": "This research paper investigates whether Large Language Models (LLMs) perform in-context learning (ICL) \u2013 learning from a few examples in their input \u2013 using Bayesian reasoning or simple pattern matching.  The researchers used a controlled experiment involving biased coin flips.  Their key findings are:\n\n1. **LLMs have biased priors:**  Even without prior instruction, LLMs start with inherent biases affecting their initial predictions (zero-shot setting).\n\n2. **In-context evidence dominates explicit bias:**  When provided with examples (few-shot learning), the in-context data overrides any explicitly stated biases.\n\n3. **LLMs largely follow Bayesian updates:** The LLMs' predictions after seeing examples align well with Bayesian posterior updates. Deviations from perfect Bayesian inference are primarily attributed to inaccurate prior beliefs (biases) rather than faulty updating mechanisms themselves.\n\n4. **Attention mechanisms are less important:** The strength of attention weights within the LLM doesn't significantly impact its Bayesian-like inference.\n\nIn essence, the study shows that despite inherent biases, LLMs can effectively leverage in-context examples to perform a type of Bayesian reasoning, refining their initial beliefs based on observed data.  The key mathematical idea is the comparison of the LLM's predictions with Bayesian posterior probability calculations, revealing a general correspondence.  The paper doesn't present specific mathematical formulas, but the comparison itself is the core mathematical result.  The deviations observed are mainly attributed to the initial prior probabilities assigned by the LLMs.\n", "  We study reconstructing and predicting 3D fluid appearance and velocity from\na single video. Current methods require multi-view videos for fluid\nreconstruction. We present FluidNexus, a novel framework that bridges video\ngeneration and physics simulation to tackle this task. Our key insight is to\nsynthesize multiple novel-view videos as references for reconstruction.\nFluidNexus consists of two key components: (1) a novel-view video synthesizer\nthat combines frame-wise view synthesis with video diffusion refinement for\ngenerating realistic videos, and (2) a physics-integrated particle\nrepresentation coupling differentiable simulation and rendering to\nsimultaneously facilitate 3D fluid reconstruction and prediction. To evaluate\nour approach, we collect two new real-world fluid datasets featuring textured\nbackgrounds and object interactions. Our method enables dynamic novel view\nsynthesis, future prediction, and interaction simulation from a single fluid\nvideo. Project website: https://yuegao.me/FluidNexus.\n": "This research paper introduces FluidNexus, a novel framework for reconstructing and predicting 3D fluid appearance and velocity from a single video \u2013 a significant advancement over existing methods that require multiple camera views.  The key innovation lies in synthesizing multiple novel views of the fluid from the single input video, effectively creating the multi-view data needed for accurate 3D reconstruction.\n\nFluidNexus achieves this through two main components:\n\n1. **Novel-view video synthesizer:** This component generates realistic, synthetic videos from novel viewpoints. It combines frame-wise view synthesis with video diffusion refinement to enhance the quality and realism of the synthesized videos.  No specific mathematical details about these techniques are provided in the abstract.\n\n2. **Physics-integrated particle representation:** This component uses a particle-based representation of the fluid, coupled with differentiable simulation and rendering.  This allows for simultaneous 3D reconstruction and prediction of the fluid's future behavior.  Again, specific mathematical formulations are absent from the abstract.\n\nThe paper highlights the evaluation of FluidNexus on two newly collected real-world datasets featuring complex scenes with textured backgrounds and fluid-object interactions.  The results demonstrate the method's ability to perform dynamic novel view synthesis, future prediction, and interaction simulation, all from a single input video.  The abstract doesn't quantify the performance or provide specific mathematical results like accuracy metrics or error bounds.  The key mathematical ideas are implicit within the described components, but remain unspecified in the abstract.\n", "  Scene flow estimation is a foundational task for many robotic applications,\nincluding robust dynamic object detection, automatic labeling, and sensor\nsynchronization. Two types of approaches to the problem have evolved: 1)\nSupervised and 2) optimization-based methods. Supervised methods are fast\nduring inference and achieve high-quality results, however, they are limited by\nthe need for large amounts of labeled training data and are susceptible to\ndomain gaps. In contrast, unsupervised test-time optimization methods do not\nface the problem of domain gaps but usually suffer from substantial runtime,\nexhibit artifacts, or fail to converge to the right solution. In this work, we\nmitigate several limitations of existing optimization-based methods. To this\nend, we 1) introduce a simple voxel grid-based model that improves over the\nstandard MLP-based formulation in multiple dimensions and 2) introduce a new\nmultiframe loss formulation. 3) We combine both contributions in our new\nmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only\nby EulerFlow among unsupervised methods while achieving comparable performance\nat a fraction of the computational cost. Floxels achieves a massive speedup of\nmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10\nminutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels\nachieves a speedup of ~14x.\n": "This research paper addresses the problem of scene flow estimation, crucial for various robotic applications.  The authors focus on improving optimization-based methods, which avoid the data limitations of supervised approaches but typically suffer from slow runtime and poor results.  Their contributions are threefold:\n\n1. **Voxel Grid-based Model:** They replace the standard multi-layer perceptron (MLP) model with a simpler, voxel grid-based representation. This improves performance across several dimensions (though the paper doesn't specify exactly which).\n\n2. **Multiframe Loss Formulation:**  A novel multiframe loss function is introduced to enhance the accuracy of the optimization process.  The details of this new loss function are not provided in the summary.\n\n3. **Floxels Method:**  Combining the voxel grid model and the new loss function, they propose a new method called Floxels.\n\n**Key Results:**  Floxels achieves state-of-the-art performance among unsupervised scene flow estimation methods on the Argoverse 2 benchmark.  Specifically, it's only outperformed by EulerFlow, but offers a significant speed advantage.  The speedup is dramatic:  ~60-140x faster than EulerFlow (reducing runtime from a day to 10 minutes per sequence), and ~14x faster than a faster but lower-quality baseline method (NSFP).  No specific mathematical equations or results are presented in the provided text.  The key mathematical improvement lies in the unspecified details of the new voxel grid model and multiframe loss function.\n", "gsplat is an open-source library designed for training and developing Gaussian Splatting methods. It features a front-end with Python bindings compatible with the PyTorch library and a back-end with highly optimized CUDA kernels. gsplat offers numerous features that enhance the optimization of Gaussian Splatting models, which include optimization improvements for speed, memory, and convergence times. Experimental results demonstrate that gsplat achieves up to 10% less training time and 4x less memory than the original implementation. Utilized in several research projects, gsplat is actively maintained on GitHub. Source code is available at https://github.com/nerfstudio-project/gsplat under Apache License 2.0. We welcome contributions from the open-source community.": "This research paper introduces gsplat, an open-source Python library built on PyTorch and CUDA for training and developing Gaussian Splatting models.  The core contribution is the improved efficiency of gsplat compared to the original Gaussian Splatting implementation.  Key improvements include:\n\n* **Faster training:** gsplat achieves up to 10% faster training times.\n* **Reduced memory consumption:** gsplat uses up to 4 times less memory during training.\n* **Improved convergence:**  While not explicitly quantified, the implication is that the optimizations lead to faster convergence to a solution.\n\nThe paper highlights gsplat's architecture, featuring a Python frontend for ease of use and a highly optimized CUDA backend for performance.  No specific mathematical formulations or results are detailed in the provided abstract; the focus is on the engineering and performance improvements offered by the library.  The key takeaway is that gsplat offers a significantly more efficient way to train Gaussian Splatting models, making the technique more accessible and scalable for research and applications.  The library's open-source nature and active maintenance on GitHub encourage community contributions.\n", "We consider online statistical inference of constrained stochastic nonlinear optimization problems. We apply the Stochastic Sequential Quadratic Programming (StoSQP) method to solve these problems, which can be regarded as applying second-order Newton's method to the Karush-Kuhn-Tucker (KKT) conditions. In each iteration, the StoSQP method computes the Newton direction by solving a quadratic program, and then selects a proper adaptive stepsize $\\bar{\\alpha}_t$ to update the primal-dual iterate. To reduce dominant computational cost of the method, we inexactly solve the quadratic program in each iteration by employing an iterative sketching solver. Notably, the approximation error of the sketching solver need not vanish as iterations proceed, meaning that the per-iteration computational cost does not blow up. For the above StoSQP method, we show that under mild assumptions, the rescaled primal-dual sequence $1/\\sqrt{\\bar{\\alpha}_t}\\cdot (x_t -x^\\star, \\lambda_t - \\lambda^\\star)$ converges to a mean-zero Gaussian distribution with a nontrivial covariance matrix depending on the underlying sketching distribution. To perform inference in practice, we also analyze a plug-in covariance matrix estimator. We illustrate the asymptotic normality result of the method both on benchmark nonlinear problems in CUTEst test set and on linearly/nonlinearly constrained regression problems.": "This research paper focuses on online statistical inference for constrained stochastic nonlinear optimization problems.  The authors employ a Stochastic Sequential Quadratic Programming (StoSQP) method, essentially a second-order Newton method applied to the Karush-Kuhn-Tucker (KKT) conditions.  The core innovation lies in using an *inexact* iterative sketching solver to solve the quadratic program in each StoSQP iteration, significantly reducing computational cost.  Crucially, the approximation error from this inexact solver doesn't need to diminish over iterations, maintaining low per-iteration cost.\n\nThe key theoretical contribution is proving the asymptotic normality of the rescaled primal-dual sequence.  Specifically, they show that  $1/\\sqrt{\\bar{\\alpha}_t}\\cdot (x_t -x^\\star, \\lambda_t - \\lambda^\\star)$ converges to a mean-zero Gaussian distribution.  The covariance matrix of this Gaussian distribution is non-trivial and depends on the sketching distribution used in the inexact solver.  A plug-in estimator for this covariance matrix is also analyzed, enabling practical inference.\n\nIn short, the paper presents a computationally efficient StoSQP method with a theoretically sound asymptotic normality result, allowing for statistical inference on the solution of constrained stochastic nonlinear optimization problems. The key mathematical result is the asymptotic normality of the rescaled iterates with a covariance matrix dependent on the sketching method, opening up possibilities for uncertainty quantification in the optimization process.  The method is validated on benchmark problems from the CUTEst test set and regression problems.\n", "While many Machine Learning methods have been developed or transposed on Riemannian manifolds to tackle data with known non-Euclidean geometry, Optimal Transport (OT) methods on such spaces have not received much attention. The main OT tool on these spaces is the Wasserstein distance, which suffers from a heavy computational burden. On Euclidean spaces, a popular alternative is the Sliced-Wasserstein distance, which leverages a closed-form solution of the Wasserstein distance in one dimension, but which is not readily available on manifolds. In this work, we derive general constructions of Sliced-Wasserstein distances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive curvature, which include among others Hyperbolic spaces or the space of Symmetric Positive Definite matrices. Then, we propose different applications such as classification of documents with a suitably learned ground cost on a manifold, and data set comparison on a product manifold. Additionally, we derive non-parametric schemes to minimize these new distances by approximating their Wasserstein gradient flows.": "This research paper addresses the computational challenges of using Optimal Transport (OT) methods, specifically the Wasserstein distance, on Riemannian manifolds\u2014non-Euclidean spaces with a defined geometry.  The Wasserstein distance, while powerful, is computationally expensive in these spaces.\n\nThe key idea is to adapt the *Sliced-Wasserstein (SW) distance*, a computationally efficient alternative to the Wasserstein distance in Euclidean spaces, to Riemannian manifolds, specifically Cartan-Hadamard manifolds (manifolds with non-positive curvature, like hyperbolic spaces or the space of Symmetric Positive Definite matrices).  The paper's main contribution is the *derivation of general constructions of SW distances on these manifolds*.  This is significant because a closed-form solution for the 1D Wasserstein distance, crucial for the SW approach, isn't readily available on manifolds.\n\nThe paper then demonstrates the practical utility of these new SW distances through:\n\n* **Applications:**  It showcases applications such as document classification (using a learned cost function on a manifold) and dataset comparison (on a product manifold).\n* **Non-parametric minimization:**  It develops non-parametric schemes to minimize the derived SW distances by approximating their Wasserstein gradient flows, providing a practical method for using them in machine learning tasks.\n\nIn essence, the paper bridges a gap in applying efficient OT methods to non-Euclidean data by extending the Sliced-Wasserstein distance to Cartan-Hadamard manifolds, and provides both theoretical foundations and practical applications.  The key mathematical result is the *novel construction and formulation of the Sliced-Wasserstein distance on Cartan-Hadamard manifolds*, enabling efficient computation of a powerful distance metric in complex data spaces.  No specific mathematical formulas are given, as the paper focuses on the general construction rather than explicit equations.\n", "The acceleration of gradient-based optimization methods is a subject of significant practical and theoretical importance, particularly within machine learning applications. While much attention has been directed towards optimizing within Euclidean space, the need to optimize over spaces of probability measures in machine learning motivates the exploration of accelerated gradient methods in this context, too. To this end, we introduce a Hamiltonian-flow approach analogous to momentum-based approaches in Euclidean space. We demonstrate that, in the continuous-time setting, algorithms based on this approach can achieve convergence rates of arbitrarily high order. We complement our findings with numerical examples.": "This research paper explores accelerated gradient-based optimization methods for optimizing over spaces of probability measures, a crucial task in machine learning.  Instead of focusing on the typical Euclidean space, the authors propose a novel approach inspired by Hamiltonian flows, analogous to momentum methods used in Euclidean spaces.  The key idea is to leverage this Hamiltonian-flow approach to design optimization algorithms that operate directly on probability measures.\n\nThe main contribution is the demonstration that, in the continuous-time limit, these Hamiltonian-flow-based algorithms can achieve arbitrarily high-order convergence rates. This is a significant improvement over existing methods. While the paper doesn't explicitly state specific mathematical formulas for the Hamiltonian or the resulting optimization algorithm, the core mathematical result is the demonstration of arbitrarily high-order convergence in the continuous-time setting \u2013 a powerful theoretical result indicating the potential for highly efficient optimization in the context of probability measures.  The paper supports this theoretical finding with numerical examples, showcasing the practical applicability of the proposed approach.  Essentially, the paper presents a new framework for accelerated optimization on probability spaces, proving its theoretical potential for high-order convergence and offering numerical evidence to support the claim.\n", "Gaussian processes are pervasive in functional data analysis, machine learning, and spatial statistics for modeling complex dependencies. Scientific data are often heterogeneous in their inputs and contain multiple known discrete groups of samples; thus, it is desirable to leverage the similarity among groups while accounting for heterogeneity across groups. We propose multi-group Gaussian processes (MGGPs) defined over $\\mathbb{R}^p\\times \\mathscr{C}$, where $\\mathscr{C}$ is a finite set representing the group label, by developing general classes of valid (positive definite) covariance functions on such domains. MGGPs are able to accurately recover relationships between the groups and efficiently share strength across samples from all groups during inference, while capturing distinct group-specific behaviors in the conditional posterior distributions. We demonstrate inference in MGGPs through simulation experiments, and we apply our proposed MGGP regression framework to gene expression data to illustrate the behavior and enhanced inferential capabilities of multi-group Gaussian processes by jointly modeling continuous and categorical variables.": "This research paper introduces Multi-Group Gaussian Processes (MGGPs) as a novel approach to modeling data with both continuous and categorical variables.  The key idea is to extend Gaussian Processes (GPs) \u2013 commonly used for modeling complex dependencies in various fields \u2013 to handle data naturally divided into distinct groups.  Instead of treating all data points identically, MGGPs leverage similarities within groups while allowing for differences between them.\n\nThe main contributions are:\n\n1. **Development of valid covariance functions:** The authors define new classes of covariance functions on the domain $\\mathbb{R}^p \\times \\mathscr{C}$, where $\\mathbb{R}^p$ represents the continuous input space and $\\mathscr{C}$ is a finite set representing the group labels.  The crucial aspect is ensuring these covariance functions are positive definite, guaranteeing a valid Gaussian process.  The paper doesn't explicitly detail the specific forms of these covariance functions, only stating they are *general classes*.\n\n2. **Efficient inference:** The MGGP framework enables efficient inference by sharing information across groups, improving the accuracy and reliability of predictions.  This \"strength sharing\" allows the model to learn from data in all groups, even when some groups have limited data.  Simultaneously, the model captures the unique characteristics of each group through group-specific behavior in the conditional posterior distributions.\n\n3. **Application to gene expression data:**  The authors demonstrate the utility of MGGPs by applying the regression framework to gene expression data, showcasing its ability to jointly model continuous gene expression levels and a categorical variable (e.g., tissue type or treatment group).  This application highlights the practical advantages of the proposed model in real-world scenarios.\n\n4. **Simulation Experiments:** The effectiveness of MGGPs is validated through simulation experiments, although specifics are not provided in the summary.\n\nIn essence, the paper proposes a more sophisticated and flexible approach to Gaussian process modeling by explicitly incorporating group structure. This leads to improved inference, particularly in situations with heterogeneous data, as demonstrated by their application to gene expression data.  The core mathematical contribution lies in the construction of valid covariance functions on the combined continuous and categorical domain, enabling the MGGP framework.  Further details about the specific forms of these covariance functions and the results of the simulation studies are absent from the provided abstract.\n", "Graph neural network (GNN) models have been widely used for learning graph-structured data. Due to the permutation-invariant requirement of graph learning tasks, a basic element in graph neural networks is the invariant and equivariant linear layers. Previous work (Maron et al., 2019b) provided a maximal collection of invariant and equivariant linear layers and a simple deep neural network model, called k-IGN, for graph data defined on k-tuples of nodes. It is shown that the expressive power of k-IGN is at least as good as the  k-Weisfeiler-Leman (WL) algorithm in graph isomorphism tests. However, the dimension of the invariant layer and equivariant layer is the k-th and 2k-th bell numbers, respectively. Such high complexity makes it computationally infeasible for k-IGNs with k >= 3. In this paper, we show that a much smaller dimension for the linear layers is sufficient to achieve the same expressive power. We provide two sets of orthogonal bases for the linear layers, each with only 3(2^k-1)-k basis elements. Based on these linear layers, we develop neural network models GNN-a and GNN-b and show that for the graph data defined on k-tuples of data, GNN-a and GNN-b achieve the expressive power of the k-WL algorithm and the (k+1)-WL algorithm in graph isomorphism tests, respectively. In molecular prediction tasks on benchmark datasets, we demonstrate that low-order neural network models consisting of the proposed linear layers achieve better performance than other neural network models. In particular, order-2 GNN-b and order-3 GNN-a both have 3-WL expressive power, but use a much smaller basis and hence much less computation time than known neural network models.": "This research paper addresses the computational limitations of existing Graph Neural Networks (GNNs) designed for graph isomorphism testing.  Specifically, it tackles the high dimensionality of the invariant and equivariant linear layers in the k-IGN model (a previous GNN), which scales with the k-th and 2k-th Bell numbers, making it impractical for k \u2265 3.\n\nThe key contribution is the development of two new GNN models, GNN-a and GNN-b, which achieve comparable expressive power with significantly reduced computational cost.  They achieve this by using novel, smaller orthogonal bases for the linear layers.  Instead of the exponentially growing Bell numbers, the dimension of their linear layers is only 3(2<sup>k</sup>-1) - k, a much more manageable size.\n\n**Key ideas and main points:**\n\n* **Problem:** High computational complexity of existing k-IGN GNNs due to large linear layer dimensions.\n* **Solution:** Proposing new orthogonal bases for the invariant and equivariant linear layers, resulting in much smaller dimensions (3(2<sup>k</sup>-1) - k).\n* **New Models:** GNN-a and GNN-b, built using these smaller bases.\n* **Expressive Power:** GNN-a achieves the expressive power of the k-Weisfeiler-Lehman (WL) algorithm, while GNN-b achieves the expressive power of the (k+1)-WL algorithm.  This means they can distinguish at least as many non-isomorphic graphs as the respective WL algorithms.\n* **Empirical Results:**  The proposed models outperform other GNNs on molecular prediction tasks, demonstrating the effectiveness of the smaller-dimension linear layers without sacrificing performance.  Specifically, lower-order GNN-a and GNN-b models (order 2 and 3) achieve 3-WL expressive power with significantly reduced computational cost.\n\n**Key mathematical idea:** The paper's core innovation lies in finding smaller orthogonal bases for the invariant and equivariant linear layers, reducing the dimensionality from Bell numbers to 3(2<sup>k</sup>-1) - k.  This is crucial for making higher-order GNNs computationally feasible.  The paper doesn't explicitly present the bases themselves, but focuses on proving their existence and the resulting dimensionality reduction and improved expressive power.\n", "Pearl\u2019s do calculus is a complete axiomatic approach to learn the identifiable causal effects from observational data. When such an effect is not identifiable, it is necessary to perform a collection of often costly interventions in the system to learn the causal effect. In this work, we consider the problem of designing a collection of interventions with the minimum cost to identify the desired effect. First, we prove that this problem is NP-complete and subsequently propose an algorithm that can either find the optimal solution or a logarithmic-factor approximation of it. This is done by establishing a connection between our problem and the minimum hitting set problem. Additionally, we propose several polynomial time heuristic algorithms to tackle the computational complexity of the problem. Although these algorithms could potentially stumble on sub-optimal solutions, our simulations show that they achieve small regrets on random graphs.": "This research paper addresses the problem of efficiently identifying causal effects from observational data using Pearl's do-calculus.  When direct identification isn't possible, interventions are necessary, but these can be costly.  The paper focuses on finding the *minimum cost* set of interventions to achieve identification.\n\n**Key Ideas and Main Points:**\n\n* **NP-completeness:** The paper proves that finding the minimum cost set of interventions to identify a causal effect is an NP-complete problem. This means finding the absolute best solution is computationally intractable for large systems.\n* **Approximation Algorithm:**  To address the NP-completeness, the authors develop an algorithm that either finds the optimal solution or provides a solution within a logarithmic factor of the optimal cost. This is achieved by linking the problem to the well-known Minimum Hitting Set problem, for which approximation algorithms exist.\n* **Heuristic Algorithms:**  Recognizing the computational limitations, the paper also proposes several heuristic algorithms that run in polynomial time.  While these might not always find the absolute best solution, simulations suggest they perform well in practice (low \"regret,\" meaning the cost of their solution is close to optimal).\n\n**Key Mathematical Ideas/Results:**\n\n* The core mathematical contribution is the reduction of the minimum-cost intervention selection problem to the Minimum Hitting Set problem. This allows leveraging existing approximation algorithms from combinatorial optimization.  The paper doesn't explicitly state the specific approximation algorithm used, but implies it's based on known algorithms for the Minimum Hitting Set problem, which typically achieve logarithmic approximation factors.\n* The NP-completeness proof establishes a formal boundary on the computational tractability of the problem, highlighting the need for approximation or heuristic approaches.\n* The simulation results, though not detailed in the summary, provide empirical evidence of the effectiveness of the proposed heuristic algorithms in achieving near-optimal solutions.\n\nIn essence, the paper tackles a crucial problem in causal inference by providing both theoretical (NP-completeness proof and approximation algorithm) and practical (heuristic algorithms and simulation results) contributions to finding cost-effective intervention strategies for causal effect identification.\n", "Robustness to malicious attacks is of paramount importance for distributed learning. Existing works usually consider the classical Byzantine attacks model, which assumes that some workers can send arbitrarily malicious messages to the server and disturb the aggregation steps of the distributed learning process. To defend against such worst-case Byzantine attacks, various robust aggregators have been proposed. They are proven to be effective and much superior to the often-used mean aggregator. In this paper, however, we demonstrate that the robust aggregators are too conservative for a class of weak but practical malicious attacks, known as label poisoning attacks, where the sample labels of some workers are poisoned. Surprisingly, we are able to show that the mean aggregator is more robust than the state-of-the-art robust aggregators in theory, given that the distributed data are sufficiently heterogeneous. In fact, the learning error of the mean aggregator is proven to be order-optimal in this case. Experimental results corroborate our theoretical findings, showing the superiority of the mean aggregator under label poisoning attacks.": "This research paper challenges the common assumption that robust aggregators (designed to withstand worst-case Byzantine attacks in distributed learning) are always superior to the simple mean aggregator.  The authors focus on a more nuanced attack, *label poisoning*, where malicious actors corrupt only the data labels at some worker nodes, rather than sending arbitrary messages.\n\n**Key Ideas and Main Points:**\n\n* **Vulnerability of Robust Aggregators:** The paper argues that robust aggregators, while effective against Byzantine attacks, are overly conservative and perform poorly against label poisoning attacks.  They are designed for the worst-case scenario, which is not representative of label poisoning.\n* **Superiority of Mean Aggregator (Under Certain Conditions):**  Surprisingly, the authors demonstrate that the simple mean aggregator is *more robust* than state-of-the-art robust aggregators when dealing with label poisoning attacks, *provided the data across workers is sufficiently heterogeneous*.  This heterogeneity acts as a natural defense mechanism against label poisoning.\n* **Theoretical Justification:** The authors provide theoretical analysis proving that the mean aggregator's learning error is order-optimal (meaning it achieves the best possible error rate up to a constant factor) under sufficiently heterogeneous data and label poisoning attacks.  This is a key mathematical result.\n* **Empirical Validation:** Experimental results support the theoretical findings, confirming the superior performance of the mean aggregator under label poisoning attacks in heterogeneous data settings.\n\n\n**Key Mathematical Idea:** The core mathematical contribution lies in the theoretical analysis showing the order-optimality of the mean aggregator's learning error under label poisoning in heterogeneous data.  While the exact mathematical expressions aren't provided in the summary, the essence is a proof demonstrating that the error incurred by the mean aggregator under these specific attack conditions scales favorably compared to robust aggregators.  This implies a superior error rate for the mean aggregator in this particular context.\n", "In this paper, we consider federated Q-learning, which aims to learn an optimal Q-function by periodically aggregating local Q-estimates trained on local data alone. Focusing on infinite-horizon tabular Markov decision processes, we provide sample complexity guarantees for both the synchronous and asynchronous variants of federated Q-learning, which exhibit a linear speedup with respect to the number of agents and near-optimal dependencies on other salient problem parameters. In the asynchronous setting, existing analyses of federated Q-learning, which adopt an equally weighted averaging of local Q-estimates, require that every agent covers the entire state-action space. In contrast, our improved sample complexity scales inverse proportionally to the minimum entry of the average stationary state-action occupancy distribution of all agents, thus only requiring the agents to collectively cover the entire state-action space, unveiling the blessing of heterogeneity. However, its sample complexity still suffers when the local trajectories are highly heterogeneous. In response, we propose a novel federated Q-learning algorithm with importance averaging, giving larger weights to more frequently visited state-action pairs, which achieves a robust linear speedup as if all trajectories are centrally processed, regardless of the heterogeneity of local behavior policies.": "This research paper analyzes the sample complexity of Federated Q-learning, a method for training a Q-function (a value function representing the expected cumulative reward from a given state-action pair) across multiple agents with decentralized data.  The focus is on infinite-horizon tabular Markov Decision Processes (MDPs).\n\n**Key Ideas and Main Points:**\n\n* **Federated Q-learning with Sample Complexity Guarantees:** The paper provides theoretical guarantees (sample complexity bounds) for both synchronous and asynchronous versions of federated Q-learning.  These bounds demonstrate a **linear speedup** in training time with respect to the number of participating agents. This means adding more agents significantly reduces the overall training time.\n\n* **Asynchronous Federated Q-learning and Heterogeneity:**  The analysis of asynchronous federated Q-learning improves upon existing work.  Previous analyses, using equally weighted averaging of local Q-estimates, required each agent to explore the entire state-action space.  This paper shows that the algorithm's sample complexity scales inversely with the minimum entry of the average stationary state-action occupancy distribution across all agents.  This means that agents only need to *collectively* cover the entire state-action space, showcasing the \"blessing of heterogeneity\" \u2013 diverse agent experiences can be beneficial.\n\n* **Importance Averaging for Robustness:**  Despite the improvement above, highly heterogeneous local trajectories still negatively impact sample complexity.  To address this, the authors propose a novel federated Q-learning algorithm with *importance averaging*. This assigns larger weights to state-action pairs visited more frequently across all agents. This modification achieves a robust linear speedup, effectively treating the data as if it were centrally processed, regardless of the heterogeneity of agent behavior.\n\n**Key Mathematical Ideas/Results:**\n\nThe core contribution lies in the derived sample complexity bounds for different variants of federated Q-learning. While the paper doesn't explicitly state the bounds, the key takeaway is the demonstration of:\n\n* **Linear speedup:**  The sample complexity scales proportionally to 1/N, where N is the number of agents.\n* **Dependence on occupancy distribution:** In the improved asynchronous analysis, the sample complexity inversely depends on the minimum entry of the average stationary state-action occupancy distribution across agents.\n* **Robustness with importance averaging:** The proposed importance averaging technique ensures a linear speedup even with highly heterogeneous agent behavior policies.  This contrasts with the previous methods, whose performance degrades significantly under high heterogeneity.\n\n\nIn essence, this paper provides a more nuanced and robust theoretical understanding of federated Q-learning, especially highlighting the benefits of agent heterogeneity when combined with a carefully designed aggregation scheme (importance averaging).  This leads to improved efficiency and scalability for decentralized reinforcement learning.\n", "PyTorch 2.x introduces a compiler designed to accelerate deep learning programs. However, for machine learning researchers, fully leveraging the PyTorch compiler can be challenging due to its operation at the Python bytecode level, making it appear as an opaque box. To address this, we introduce depyf, a tool designed to demystify the inner workings of the PyTorch compiler. depyf decompiles the bytecode generated by PyTorch back into equivalent source code and establishes connections between the code objects in the memory and their counterparts in source code format on the disk. This feature enables users to step through the source code line by line using debuggers, thus enhancing their understanding of the underlying processes. Notably, depyf is non-intrusive and user-friendly, primarily relying on two convenient context managers for its core functionality. The project is openly available at https://github.com/thuml/depyf and is recognized as a PyTorch ecosystem project at https://pytorch.org/blog/introducing-depyf.": "This research paper introduces depyf, a tool designed to improve understanding and debugging of the PyTorch 2.x compiler.  The PyTorch compiler operates at the Python bytecode level, making its internal workings difficult for researchers to analyze. Depyf addresses this by:\n\n* **Decompiling bytecode:** It translates the bytecode generated by the PyTorch compiler back into equivalent Python source code.  This makes the compilation process much more transparent.\n\n* **Connecting code objects:**  It links in-memory code objects to their corresponding source code files on disk. This allows users to directly trace the execution path within their original code.\n\n* **Enabling debugging:** The decompiled code can be stepped through using standard debuggers, allowing researchers to understand how the compiler transforms and optimizes their code.\n\nThe key innovation is the user-friendly design.  Depyf achieves its functionality primarily through two context managers, minimizing user intervention and making it easy to integrate into existing workflows.  No specific mathematical results are presented as the paper focuses on a tool for improving the understanding and debugging of a compiler, not on developing new mathematical algorithms within deep learning itself.  The core contribution is a tool (available on GitHub) that bridges the gap between high-level PyTorch code and the lower-level compiler operations, making the optimization process more accessible to researchers.\n"}